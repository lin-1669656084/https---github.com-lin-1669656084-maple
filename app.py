# app.py - Flaskåç«¯åº”ç”¨
import os
import uuid
from flask import Flask, request, render_template, send_from_directory, jsonify
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import base64
import io

app = Flask(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
model = None
device = None
cfg = None
model_info = {}


def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹"""
    global model, device, cfg, model_info

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ¨¡å‹æ–‡ä»¶ä¼˜å…ˆçº§åˆ—è¡¨
    model_paths = [
        ("enhanced_maple_best.pth", "enhanced"),
        ("enhanced_maple_final.pth", "enhanced"),
        ("maple_mnist_state_dict.pth", "simple")
    ]

    for model_path, model_type in model_paths:
        if os.path.exists(model_path):
            try:
                if model_type == "enhanced":
                    # å°è¯•å¯¼å…¥å¢å¼ºæ¨¡å‹
                    try:
                        from enhanced_maple import EnhancedMaPLe, EnhancedConfig
                        cfg = EnhancedConfig()
                        model = EnhancedMaPLe(
                            input_dim=28 * 28,
                            num_classes=10,
                            prompt_dim=cfg.prompt_dim,
                            mask_ratio=cfg.mask_ratio,
                            num_layers=cfg.num_layers,
                            dropout_rate=cfg.dropout_rate,
                            use_attention=cfg.use_attention,
                            use_residual=cfg.use_residual
                        ).to(device)

                        # åŠ è½½æ¨¡å‹æƒé‡
                        checkpoint = torch.load(model_path, map_location=device)
                        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                            model.load_state_dict(checkpoint['model_state_dict'])
                            if 'best_accuracy' in checkpoint:
                                model_info['best_accuracy'] = checkpoint['best_accuracy']
                        else:
                            model.load_state_dict(checkpoint)

                        model_info['type'] = 'EnhancedMaPLe'
                        print(f"âœ… æˆåŠŸåŠ è½½å¢å¼ºæ¨¡å‹: {model_path}")

                    except ImportError:
                        print("âŒ æ— æ³•å¯¼å…¥å¢å¼ºæ¨¡å‹ï¼Œå°è¯•ç®€å•æ¨¡å‹...")
                        continue

                elif model_type == "simple":
                    # ä½¿ç”¨ä½ æä¾›çš„åŸå§‹MaPLeæ¨¡å‹
                    import torch.nn as nn

                    class MaPLe(nn.Module):
                        def __init__(self, input_dim=28 * 28, num_classes=10, prompt_dim=100, mask_ratio=0.5):
                            super(MaPLe, self).__init__()
                            self.prompt_dim = prompt_dim
                            self.mask_ratio = mask_ratio
                            self.embedding = nn.Linear(input_dim, prompt_dim)
                            self.prompt_mask = nn.Parameter(torch.ones(prompt_dim), requires_grad=True)
                            self.classifier = nn.Linear(prompt_dim, num_classes)

                        def forward(self, x):
                            B = x.size(0)
                            x = x.view(B, -1)  # flatten
                            x = self.embedding(x)  # project to prompt space

                            # Apply learnable mask
                            mask = torch.sigmoid(self.prompt_mask)  # values between 0 and 1
                            masked_x = x * mask  # element-wise multiplication

                            logits = self.classifier(masked_x)
                            return logits

                    model = MaPLe(prompt_dim=100, mask_ratio=0.5).to(device)
                    model.load_state_dict(torch.load(model_path, map_location=device))

                    # åˆ›å»ºç®€å•é…ç½®
                    class SimpleConfig:
                        def __init__(self):
                            self.prompt_dim = 100
                            self.mask_ratio = 0.5
                            self.num_layers = 1
                            self.dropout_rate = 0.0

                    cfg = SimpleConfig()
                    model_info['type'] = 'SimpleMaPLe'
                    print(f"âœ… æˆåŠŸåŠ è½½ç®€å•æ¨¡å‹: {model_path}")

                model.eval()

                # è®¡ç®—æ¨¡å‹å‚æ•°ä¿¡æ¯
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

                model_info.update({
                    'total_params': total_params,
                    'trainable_params': trainable_params,
                    'device': str(device),
                    'loaded_from': model_path
                })

                return True

            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")
                continue

    print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
    print("è¿è¡Œå‘½ä»¤: python enhanced_train.py æˆ– python train.py")
    return False


# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
model_loaded = load_model()

# åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹
UPLOAD_FOLDER = 'uploads'
TEMPLATE_FOLDER = 'templates'
for folder in [UPLOAD_FOLDER, TEMPLATE_FOLDER]:
    if not os.path.exists(folder):
        os.makedirs(folder)


def preprocess_image(image_path=None, image_data=None):
    """é¢„å¤„ç†å›¾åƒï¼Œæ”¯æŒæ–‡ä»¶è·¯å¾„æˆ–base64æ•°æ®"""
    if image_path:
        img = Image.open(image_path).convert('L')
    elif image_data:
        # å¤„ç†base64æ•°æ®
        if ',' in image_data:
            image_data = image_data.split(',')[1]  # ç§»é™¤data:image/png;base64,å‰ç¼€
        img_bytes = base64.b64decode(image_data)
        img = Image.open(io.BytesIO(img_bytes)).convert('L')
    else:
        raise ValueError("å¿…é¡»æä¾›image_pathæˆ–image_data")

    # è°ƒæ•´å¤§å°åˆ°28x28
    img = img.resize((28, 28), Image.Resampling.LANCZOS)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    img_array = np.array(img)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åè½¬é¢œè‰²ï¼ˆMNISTæ˜¯é»‘åº•ç™½å­—ï¼‰
    if np.mean(img_array) > 127:
        img_array = 255 - img_array

    # æ ‡å‡†åŒ–åˆ°[0,1]èŒƒå›´
    img_array = img_array.astype(np.float32) / 255.0

    # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
    img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)

    return img_tensor


@app.route('/uploads/<filename>')
def uploaded_file(filename):
    """æä¾›ä¸Šä¼ æ–‡ä»¶çš„è®¿é—®"""
    return send_from_directory(UPLOAD_FOLDER, filename)


@app.route('/')
def index():
    """ä¸»é¡µ - ç›´æ¥è¿”å›HTMLå†…å®¹"""
    # å¦‚æœæ²¡æœ‰templatesæ–‡ä»¶å¤¹æˆ–æ¨¡æ¿æ–‡ä»¶ï¼Œç›´æ¥è¿”å›HTML
    template_path = os.path.join(TEMPLATE_FOLDER, 'enhanced_index.html')

    if not os.path.exists(template_path):
        # åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
        html_content = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MaPLe æ‰‹å†™æ•°å­—è¯†åˆ«</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        .container { text-align: center; }
        .status { padding: 10px; margin: 10px; border-radius: 5px; }
        .success { background-color: #d4edda; color: #155724; }
        .error { background-color: #f8d7da; color: #721c24; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§  MaPLe æ‰‹å†™æ•°å­—è¯†åˆ«</h1>
        <div class="status {status_class}">
            {status_message}
        </div>
        <p>è¯·è®¿é—®å®Œæ•´ç‰ˆç•Œé¢æˆ–é€šè¿‡APIæ¥å£ä½¿ç”¨æ¨¡å‹</p>
        <div>
            <h3>APIæ¥å£:</h3>
            <ul>
                <li>POST /predict - é¢„æµ‹æ¥å£</li>
                <li>GET /model_info - æ¨¡å‹ä¿¡æ¯</li>
            </ul>
        </div>
    </div>
</body>
</html>"""

        status_class = "success" if model_loaded else "error"
        status_message = "âœ… æ¨¡å‹å·²åŠ è½½" if model_loaded else "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹"

        return html_content.format(
            status_class=status_class,
            status_message=status_message
        )

    try:
        return render_template('enhanced_index.html', model_loaded=model_loaded)
    except:
        # å¤‡ç”¨ç®€å•é¡µé¢
        return f"""
        <html>
            <head><title>MaPLe æ‰‹å†™æ•°å­—è¯†åˆ«</title></head>
            <body style="font-family: Arial; text-align: center; padding: 50px;">
                <h1>ğŸ§  MaPLe æ‰‹å†™æ•°å­—è¯†åˆ«</h1>
                <p>æ¨¡å‹çŠ¶æ€: {'âœ… å·²åŠ è½½' if model_loaded else 'âŒ æœªåŠ è½½'}</p>
                <p>è¯·å°†ä¸Šé¢æä¾›çš„HTMLä»£ç ä¿å­˜ä¸º templates/enhanced_index.html</p>
            </body>
        </html>
        """


@app.route('/predict', methods=['POST'])
def predict():
    """é¢„æµ‹API - æ”¯æŒæ–‡ä»¶ä¸Šä¼ å’Œç”»å¸ƒæ•°æ®"""
    if not model_loaded:
        return jsonify({
            'success': False,
            'error': 'æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹'
        })

    try:
        # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
        if 'image' in request.files:
            file = request.files['image']
            if file and file.filename != '':
                # ä¿å­˜æ–‡ä»¶
                filename = str(uuid.uuid4()) + os.path.splitext(file.filename)[1]
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                file.save(filepath)

                # é¢„å¤„ç†å›¾åƒ
                img_tensor = preprocess_image(image_path=filepath)
                image_url = f'/uploads/{filename}'
            else:
                return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'})

        # å¤„ç†ç”»å¸ƒæ•°æ®
        elif request.is_json and 'canvas_data' in request.json:
            canvas_data = request.json['canvas_data']
            img_tensor = preprocess_image(image_data=canvas_data)
            image_url = None

        else:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›å›¾åƒæ•°æ®'})

        # æ¨¡å‹é¢„æµ‹
        img_tensor = img_tensor.to(device)

        with torch.no_grad():
            outputs = model(img_tensor)

            # å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
            if isinstance(outputs, tuple):
                logits = outputs[0]
            else:
                logits = outputs

            # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
            probabilities = F.softmax(logits, dim=1)
            prediction = logits.argmax(dim=1).item()
            confidence = probabilities.max().item()

            # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
            all_probs = probabilities[0].cpu().numpy().tolist()

        return jsonify({
            'success': True,
            'prediction': int(prediction),
            'confidence': round(confidence * 100, 2),
            'probabilities': [round(p * 100, 2) for p in all_probs],
            'image_url': image_url
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'
        })


@app.route('/model_info')
def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if not model_loaded:
        return jsonify({
            'loaded': False,
            'error': 'æ¨¡å‹æœªåŠ è½½'
        })

    return jsonify({
        'loaded': True,
        'model_type': model_info.get('type', 'Unknown'),
        'total_parameters': model_info.get('total_params', 0),
        'trainable_parameters': model_info.get('trainable_params', 0),
        'device': model_info.get('device', 'unknown'),
        'loaded_from': model_info.get('loaded_from', 'unknown'),
        'best_accuracy': model_info.get('best_accuracy', 'N/A'),
        'config': {
            'prompt_dim': cfg.prompt_dim if cfg else 'N/A',
            'mask_ratio': cfg.mask_ratio if cfg else 'N/A',
            'num_layers': getattr(cfg, 'num_layers', 'N/A'),
            'dropout_rate': getattr(cfg, 'dropout_rate', 'N/A')
        }
    })


@app.route('/train', methods=['POST'])
def start_training():
    """å¯åŠ¨è®­ç»ƒæç¤º"""
    return jsonify({
        'success': True,
        'message': 'è¯·åœ¨å‘½ä»¤è¡Œè¿è¡Œè®­ç»ƒè„šæœ¬',
        'instructions': [
            'å¢å¼ºæ¨¡å‹: python enhanced_train.py',
            'ç®€å•æ¨¡å‹: python train.py',
            'ç„¶åé‡æ–°å¯åŠ¨Flaskåº”ç”¨'
        ]
    })


@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'ok',
        'model_loaded': model_loaded,
        'device': str(device) if device else 'unknown',
        'timestamp': str(torch.initial_seed())
    })


@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({
        'error': 'Not Found',
        'message': 'è¯·è®¿é—® / è·å–ä¸»é¡µ'
    }), 404


@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    return jsonify({
        'error': 'Internal Server Error',
        'message': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'
    }), 500


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨Flaskåº”ç”¨...")
    print(f"ğŸ“± æ¨¡å‹åŠ è½½çŠ¶æ€: {'âœ… æˆåŠŸ' if model_loaded else 'âŒ å¤±è´¥'}")

    if model_loaded:
        print(f"ğŸ§  æ¨¡å‹ç±»å‹: {model_info.get('type', 'Unknown')}")
        print(f"ğŸ’¾ å‚æ•°æ•°é‡: {model_info.get('total_params', 0):,}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {model_info.get('device', 'unknown')}")
    else:
        print("âš ï¸  è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è®­ç»ƒæ¨¡å‹:")
        print("   python enhanced_train.py  # å¢å¼ºæ¨¡å‹")
        print("   python train.py          # ç®€å•æ¨¡å‹")

    print("ğŸŒ è®¿é—® http://127.0.0.1:5000 å¼€å§‹ä½¿ç”¨")
    print("ğŸ“¡ APIæ¥å£:")
    print("   POST /predict     - é¢„æµ‹æ¥å£")
    print("   GET  /model_info  - æ¨¡å‹ä¿¡æ¯")
    print("   GET  /health      - å¥åº·æ£€æŸ¥")

    app.run(debug=True, host='127.0.0.1', port=5000)